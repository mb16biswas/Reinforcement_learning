{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10b57fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym \n",
    "import random\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from rl.agents import DQNAgent\n",
    "from rl.policy import BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d899bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4772130d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        # Actions we can take, down, stay, up\n",
    "        self.action_space = Discrete(3)\n",
    "        # Temperature array\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]))\n",
    "        # Set start temp\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Set shower length\n",
    "        self.shower_length = 60\n",
    "        \n",
    "    def step(self, action):\n",
    "        # Apply action\n",
    "        # 0 -1 = -1 temperature\n",
    "        # 1 -1 = 0 \n",
    "        # 2 -1 = 1 temperature \n",
    "        self.state += action -1 \n",
    "        # Reduce shower length by 1 second\n",
    "        self.shower_length -= 1 \n",
    "        \n",
    "        # Calculate reward\n",
    "        if self.state >=37 and self.state <=39: \n",
    "            reward =1 \n",
    "        else: \n",
    "            reward = -1 \n",
    "        \n",
    "        # Check if shower is done\n",
    "        if self.shower_length <= 0: \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        \n",
    "        # Apply temperature noise\n",
    "        #self.state += random.randint(-1,1)\n",
    "        # Set placeholder for info\n",
    "        info = {}\n",
    "        \n",
    "        # Return step information\n",
    "        return self.state, reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        # Implement viz\n",
    "        pass\n",
    "    \n",
    "    def reset(self):\n",
    "        # Reset shower temperature\n",
    "        self.state = 38 + random.randint(-3,3)\n",
    "        # Reset shower time\n",
    "        self.shower_length = 60 \n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3819ba",
   "metadata": {},
   "source": [
    "The observation_space defines the structure of the observations your environment will be returning. Learning agents usually need to know this before they start running, in order to set up the policy function. Some general-purpose learning agents can handle a wide range of observation types: Discrete, Box, or pixels (which is usually a Box(0, 255, [height, width, 3]) for RGB pixels).\n",
    "\n",
    "See at https://gym.openai.com/docs under Spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d95363e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MB16\\reinforce\\envs\\lib\\site-packages\\gym\\logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env = ShowerEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51d4f12b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.ShowerEnv at 0x4181841d00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e11d537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:-60\n",
      "Episode:2 Score:-34\n",
      "Episode:3 Score:-30\n",
      "Episode:4 Score:-34\n",
      "Episode:5 Score:-2\n",
      "Episode:6 Score:2\n",
      "Episode:7 Score:16\n",
      "Episode:8 Score:-48\n",
      "Episode:9 Score:-60\n",
      "Episode:10 Score:-44\n"
     ]
    }
   ],
   "source": [
    "episodes = 10\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    \n",
    "    while not done:\n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "        n_state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "    print('Episode:{} Score:{}'.format(episode, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a376a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = env.observation_space.shape[0]\n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a74d950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "states , actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f46a51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box(0.0, 100.0, (1,), float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bc534ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(states, actions):\n",
    "    model = Sequential() \n",
    "    model.add(Flatten(input_shape=[1]))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(24, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b1df958a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(states, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b56f75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 1)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 24)                48        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 24)                600       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 3)                 75        \n",
      "=================================================================\n",
      "Total params: 723\n",
      "Trainable params: 723\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "765c649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    memory = SequentialMemory(limit=50000, window_length=1)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy, \n",
    "                  nb_actions=actions, nb_steps_warmup=10, target_model_update=1e-2)\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cdd6569",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = build_agent(model, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9ba7a6d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 50000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From C:\\Users\\MB16\\reinforce\\envs\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "    1/10000 [..............................] - ETA: 10:50 - reward: 1.0000"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MB16\\reinforce\\envs\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 48s 5ms/step - reward: -0.4382\n",
      "166 episodes - episode_reward: -26.386 [-60.000, 32.000] - loss: 1.433 - mae: 7.910 - mean_q: -11.171\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 48s 5ms/step - reward: -0.2792\n",
      "167 episodes - episode_reward: -16.719 [-60.000, 44.000] - loss: 1.368 - mae: 7.614 - mean_q: -10.689\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 49s 5ms/step - reward: 0.3508\n",
      "167 episodes - episode_reward: 20.994 [-52.000, 60.000] - loss: 1.036 - mae: 5.157 - mean_q: 1.162\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 50s 5ms/step - reward: 0.8126\n",
      "166 episodes - episode_reward: 48.759 [26.000, 60.000] - loss: 9.758 - mae: 22.465 - mean_q: 34.233\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 51s 5ms/step - reward: 0.7616\n",
      "done, took 245.342 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4197eadd00>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "dqn.fit(env, nb_steps=50000, visualize=False, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4d1ee5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 15 episodes ...\n",
      "Episode 1: reward: 60.000, steps: 60\n",
      "Episode 2: reward: 60.000, steps: 60\n",
      "Episode 3: reward: 60.000, steps: 60\n",
      "Episode 4: reward: 60.000, steps: 60\n",
      "Episode 5: reward: 58.000, steps: 60\n",
      "Episode 6: reward: 60.000, steps: 60\n",
      "Episode 7: reward: 58.000, steps: 60\n",
      "Episode 8: reward: 60.000, steps: 60\n",
      "Episode 9: reward: 58.000, steps: 60\n",
      "Episode 10: reward: 60.000, steps: 60\n",
      "Episode 11: reward: 58.000, steps: 60\n",
      "Episode 12: reward: 58.000, steps: 60\n",
      "Episode 13: reward: 60.000, steps: 60\n",
      "Episode 14: reward: 58.000, steps: 60\n",
      "Episode 15: reward: 60.000, steps: 60\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x4197e49430>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=15, visualize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e756e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
